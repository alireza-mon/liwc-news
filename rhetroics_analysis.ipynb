{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to do the analysis on combined dimensions like Appenddic D of the paper\n",
    "# set the following variable to the extra dimension you want to analyze otherwise set it to None\n",
    "COMBINED_DIMENSION = None#'risk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bias_groups = ['Left','Left-Center','Least Biased','Right-Center','Right']\n",
    "sorted_reliability_groups = [\"Non-misinformation\",\"Misinformation\"]\n",
    "GENRATE_ALL_FIGURES = True\n",
    "\n",
    "\n",
    "bias_color_dict = {\n",
    "    'Left': (0.1, 0.1, 0.7),  # Blue, kept dark to distinguish from lighter blue\n",
    "    'Left-Center': (0.2, 0.4, 0.8),  # Light blue, more distinguishable from the darker blue\n",
    "    'Least Biased': (0.75, 0.75, 0.75),  # Medium gray, to ensure good contrast with both blues and reds\n",
    "    'Right-Center': (0.95, 0.55, 0.55),  # Soft red, distinguishable from a more intense red\n",
    "    'Right': (0.8, 0.1, 0.1)  # Red, kept deep to distinguish from the softer red\n",
    "}\n",
    "\n",
    "\n",
    "reliability_color_dict = {\n",
    "    'Non-misinformation': (0.4, 0.8, 0.4),  # Light Green\n",
    "    'Misinformation': (0.5, 0, 0.25)        # Maroon\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_potential_twitter_accounts = os.listdir(\"./data/sorted_tweets_pickles_1_13\")\n",
    "all_potential_twitter_accounts = [file[:-3] for file in all_potential_twitter_accounts]    \n",
    "all_potential_twitter_accounts.sort()\n",
    "len(all_potential_twitter_accounts)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the dataframe for saving time\n",
    "# If you want to save the dataframe for later use, set this variable to True\n",
    "delete_the_saved_dataframes = True\n",
    "if delete_the_saved_dataframes:\n",
    "    ! rm ./data/publisher_and_tweets_df.pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The liwc results are in the following file tweetid is a column in the file which is used to merge the dataframes\n",
    "tweets_liwc_results_file = \"data/tweet_level_1_13_texts/LIWC_22_combined.csv\"\n",
    "tweets_liwc_results_df = pd.read_csv(tweets_liwc_results_file)\n",
    "# rename the Filename column to tweet_id\n",
    "tweets_liwc_results_df.rename(columns={\"Filename\": \"tweet_id\"}, inplace=True)\n",
    "# lower case the column names\n",
    "tweets_liwc_results_df.columns = map(str.lower, tweets_liwc_results_df.columns)\n",
    "tweets_liwc_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the mean of the wc,WPS,analytic,authentic,cogproc,Certitude,discrep,moral,socbehav,conflict,female,relig,sexual,risk,reward,curiosity,focuspast,focuspresent,focusfuture colunms\n",
    "target_liwc_columns = [\"wc\",\"wps\",\"analytic\", \"cogproc\", \"socbehav\", \"risk\", \"reward\",\"authentic\",  \"curiosity\", \"perception\",\"clout\"]\n",
    "target_liwc_columns = [\"analytic\", \"clout\",\"perception\",\"risk\"]\n",
    "#target_liwc_columns = [\"analytic\"]\n",
    "\n",
    "\n",
    "target_liwc_medians = dict()\n",
    "for tc in target_liwc_columns:\n",
    "    target_liwc_medians[tc] = tweets_liwc_results_df[tc].median()\n",
    "target_liwc_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the tweets_liwc_results_df to the target columns\n",
    "tweets_liwc_results_df = tweets_liwc_results_df[target_liwc_columns + [\"tweet_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_CDF_CCDFS = False\n",
    "if PLOT_CDF_CCDFS:\n",
    "    for tc in target_liwc_columns:\n",
    "        plot_cdf_ccdf(tweets_liwc_results_df[tc],xlabel=f\"{tc}\",ylabel=\"CDF\",file_name=f\"{tc}_cdf_ccdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_liwc_results_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# how many of the tweet_ids in tweets_df are in the tweets_liwc_results_df\n",
    "all_tweet_ids_in_tweets_df = set(tweets_df.tweet_id.values)\n",
    "all_tweet_ids_in_tweets_liwc_results_df = set(tweets_liwc_results_df.tweet_id.values)\n",
    "len(all_tweet_ids_in_tweets_df.intersection(all_tweet_ids_in_tweets_liwc_results_df)),len(set(tweets_df.tweet_id.values)),len(set(tweets_liwc_results_df.tweet_id.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_tweet_text_column = True\n",
    "if not os.path.exists(\"./data/publisher_and_tweets_df.pk\"):\n",
    "    column_names = [\"tweet_id\", \"tweet_creation_date\", \"tweet_text\", \"like_count\", \"retweet_count\", \n",
    "                    \"quote_count\", \"reply_count\", \"bookmark_count\", \"impression_count\", \"followers_count\"]\n",
    "    tweets_df_list = []\n",
    "    for next_twitter_account in tqdm(all_potential_twitter_accounts):\n",
    "        next_twitter_account_list_of_tweets = un_pickelize(\"./data/sorted_tweets_pickles_1_13/\" + next_twitter_account + \".pk\")\n",
    "        next_twitter_account_df = pd.DataFrame(next_twitter_account_list_of_tweets, columns=column_names)\n",
    "        if drop_tweet_text_column:\n",
    "            next_twitter_account_df.drop(columns=[\"tweet_text\"], inplace=True)\n",
    "        \n",
    "\n",
    "            \n",
    "        \n",
    "        # add the next_twitter_account as the last column and put next_twitter_account in all rows\n",
    "        next_twitter_account_df['twitter_account'] = next_twitter_account\n",
    "        tweets_df_list.append(next_twitter_account_df)\n",
    "\n",
    "    tweets_df = pd.concat(tweets_df_list)\n",
    "    del tweets_df_list\n",
    "    # join the tweets_df and tweets_liwc_results_df to tweets_df and drop the rows from tweets_df that don't have a corresponding row in tweets_liwc_results_df\n",
    "    tweets_df[\"total_enagement\"] = tweets_df[\"like_count\"] + tweets_df[\"retweet_count\"] + tweets_df[\"quote_count\"] + tweets_df[\"reply_count\"]\n",
    "    #tweets_df[\"deep_count\"] = tweets_df[\"retweet_count\"] + tweets_df[\"quote_count\"] + tweets_df[\"reply_count\"]\n",
    "    #print(\"Number of tweets without any views is:\",tweets_df[tweets_df[\"impression_count\"]==0].shape)\n",
    "    tweets_df = tweets_df[tweets_df['impression_count'] > 0]# limit to tweets with views\n",
    "    tweets_df[\"engagement_rate\"] = tweets_df[\"total_enagement\"] / tweets_df[\"impression_count\"]\n",
    "    print(\"Number of tweets with engagement rate > 1 is:\",tweets_df[tweets_df[\"engagement_rate\"]>1].shape)\n",
    "    tweets_df = tweets_df[tweets_df['engagement_rate'] < 1]\n",
    "    #for next_column in ['like', 'retweet', 'quote', 'reply', 'deep' ]:\n",
    "    #    tweets_df[f\"{next_column}_engagement_rate\"] = tweets_df[f\"{next_column}_count\"] / tweets_df['impression_count']\n",
    "            \n",
    "    tweets_df['tweet_id'] = pd.to_numeric(tweets_df['tweet_id'], errors='coerce')\n",
    "    tweets_df = tweets_df.merge(tweets_liwc_results_df, on='tweet_id', how='inner')\n",
    "    #del tweets_liwc_results_df\n",
    "    \n",
    "    \n",
    "    COMPUTE_PUBLISHERS_DF = True\n",
    "    if COMPUTE_PUBLISHERS_DF:\n",
    "        publishers_df = pd.read_csv(\"../Twitter_td/df_final_web.csv\")\n",
    "        publishers_df = publishers_df[[ 'Bias', 'Reliability','twitter_account', 'followers_count']]\n",
    "        publishers_df = publishers_df[publishers_df[\"followers_count\"] > 10000]\n",
    "        #'Credibility', 'source', 'link', 'final_source',l\n",
    "        publishers_df.columns = ['Bias', 'Reliability','twitter_account', 'followers_count_march']\n",
    "        tweets_df = tweets_df.merge(publishers_df, on='twitter_account', how='inner')\n",
    "\n",
    "        #tweets_df = tweets_df[~tweets_df['twitter_account'].isin(twitter_account_counts[twitter_account_counts < 100].index)]\n",
    "        \n",
    "        \n",
    "        publishers_df = tweets_df.groupby('twitter_account').agg({'Bias': 'first', 'Reliability': 'first',\n",
    "                                                                'engagement_rate':'median',\n",
    "                                                                'like_count' : 'sum',\n",
    "                                                                'retweet_count' : 'sum',\n",
    "                                                                'quote_count' : 'sum',\n",
    "                                                                'reply_count' : 'sum',\n",
    "                                                                #'like_engagement_rate':'median',\n",
    "                                                                #'retweet_engagement_rate':'median',\n",
    "                                                                #'quote_engagement_rate':'median',\n",
    "                                                                #'reply_engagement_rate':'median',\n",
    "                                                                'tweet_id': 'count', \n",
    "                                                                'followers_count': 'mean', 'followers_count_march':\"first\",\n",
    "                                                                'total_enagement': 'sum','impression_count':'sum' ,}).reset_index()\n",
    "        #publishers_df.columns = ['twitter_account','Bias', 'Reliability','engagement_rate','like_count','retweet_count','quote_count','reply_count','like_engagement_rate','retweet_engagement_rate','quote_engagement_rate','reply_engagement_rate', 'number_of_tweets', 'followers_count_mean','followers_count_march', 'total_enagement', 'impression_count']\n",
    "        publishers_df.columns = ['twitter_account','Bias', 'Reliability','engagement_rate','like_count','retweet_count','quote_count','reply_count', 'number_of_tweets', 'followers_count_mean','followers_count_march', 'total_enagement', 'impression_count']\n",
    "        # apply int to the followers_count_mean column\n",
    "        publishers_df['followers_count_mean'] = publishers_df['followers_count_mean'].astype(int)\n",
    "        # renamed the columnn followers_count_march to followers_count\n",
    "        publishers_df.rename(columns={'followers_count_march':'followers_count'}, inplace=True)\n",
    "\n",
    "\n",
    "        for liwc_feature in target_liwc_columns:\n",
    "            \n",
    "            if COMBINED_DIMENSION == None:\n",
    "                temp = tweets_df[tweets_df[liwc_feature] >target_liwc_medians[liwc_feature]].groupby('twitter_account').agg({'tweet_id': 'count'})\n",
    "            else:\n",
    "                temp = tweets_df[(tweets_df[liwc_feature] >target_liwc_medians[liwc_feature])&(tweets_df[COMBINED_DIMENSION] >target_liwc_medians[COMBINED_DIMENSION])].groupby('twitter_account').agg({'tweet_id': 'count'})\n",
    "            temp.columns = [f'high_{liwc_feature}_tweets']\n",
    "            # Use an outer join instead of inner\n",
    "            publishers_df = publishers_df.merge(temp, on='twitter_account', how='outer')\n",
    "            # Replace NaN values with 0 in the liwc_feature tweets column\n",
    "            publishers_df[f'high_{liwc_feature}_tweets'].fillna(0, inplace=True) #######################\n",
    "            # Calculate the percentage; ensure division by zero is handled\n",
    "            publishers_df[f'high_{liwc_feature}_percentage'] = publishers_df[f'high_{liwc_feature}_tweets']*100 / publishers_df['number_of_tweets']\n",
    "            # drop the f'{sentiment}_tweets' column \n",
    "            assert(publishers_df[f'high_{liwc_feature}_tweets'].min() >= 0)\n",
    "\n",
    "            publishers_df.drop(columns=[f'high_{liwc_feature}_tweets'], inplace=True)\n",
    "            \n",
    "            if COMBINED_DIMENSION == None:\n",
    "                temp = tweets_df[tweets_df[liwc_feature] >target_liwc_medians[liwc_feature]].groupby('twitter_account').agg({'engagement_rate': 'median'})\n",
    "            else:\n",
    "                temp = tweets_df[(tweets_df[liwc_feature] >target_liwc_medians[liwc_feature])&(tweets_df[COMBINED_DIMENSION] >target_liwc_medians[COMBINED_DIMENSION])].groupby('twitter_account').agg({'engagement_rate': 'median'})\n",
    "            temp.columns = [f'high_{liwc_feature}_engagement_rate']\n",
    "            # Use an outer join instead of inner    \n",
    "            publishers_df = publishers_df.merge(temp, on='twitter_account', how='outer')\n",
    "\n",
    "            \n",
    "\n",
    "        pickelize((publishers_df,tweets_df), \"./data/publisher_and_tweets_df.pk\")\n",
    "else:\n",
    "    publishers_df,tweets_df = un_pickelize(\"./data/publisher_and_tweets_df.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns for each of the liwc features to the tweets_df so that shows if the tweet is above or below the median for that feature\n",
    "if COMBINED_DIMENSION == None:\n",
    "    for liwc_feature in target_liwc_columns:\n",
    "        tweets_df[f'high_{liwc_feature}'] = tweets_df[liwc_feature] > target_liwc_medians[liwc_feature]\n",
    "        tweets_df[f'high_{liwc_feature}'] = tweets_df[f'high_{liwc_feature}'].astype(int)\n",
    "else:\n",
    "    for liwc_feature in target_liwc_columns:\n",
    "        tweets_df[f'high_{liwc_feature}'] = ((tweets_df[liwc_feature] > target_liwc_medians[liwc_feature])&(tweets_df[COMBINED_DIMENSION] >target_liwc_medians[COMBINED_DIMENSION]))\n",
    "        tweets_df[f'high_{liwc_feature}'] = tweets_df[f'high_{liwc_feature}'].astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming tweets_df, publishers_df, target_liwc_columns, and target_liwc_medians are already defined\n",
    "\n",
    "# Pre-computation of unique twitter accounts and their engagement rates\n",
    "NOT_NEEDED = True\n",
    "if not NOT_NEEDED:\n",
    "    unique_twitter_accounts = publishers_df[\"twitter_account\"].unique()\n",
    "    engagement_rates = {account: temp[temp[\"twitter_account\"] == account][\"engagement_rate_tweets\"] for account in unique_twitter_accounts}\n",
    "\n",
    "    # Vectorized operation for percentile calculation\n",
    "    candidate_percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "    # Function to calculate the shift average\n",
    "    def calculate_shift_average(publisher, liwc_feature):\n",
    "        original_engagement_rates = engagement_rates[publisher]\n",
    "        high_analytic_engagement_rates = temp[(temp[\"twitter_account\"] == publisher) & (temp[liwc_feature] > target_liwc_medians[liwc_feature])][\"engagement_rate_tweets\"]\n",
    "        if high_analytic_engagement_rates.shape[0] == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        sum_percentiles = sum(\n",
    "            stats.percentileofscore(original_engagement_rates, np.percentile(high_analytic_engagement_rates, per)) - per\n",
    "            for per in candidate_percentiles\n",
    "        )\n",
    "        return sum_percentiles / len(candidate_percentiles)\n",
    "\n",
    "    # Applying the function over the DataFrame\n",
    "    for liwc_feature in target_liwc_columns:\n",
    "        publishers_df[\"all_percentiles_shift_average_\" + liwc_feature] = publishers_df[\"twitter_account\"].apply(lambda x: calculate_shift_average(x, liwc_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the publishers_df and tweets_df to tweets_df and drop the rows from tweets_df that don't have a corresponding row in publishers_df\n",
    "COMPUTE_AVG = False\n",
    "if not COMPUTE_AVG:\n",
    "    temp = pd.merge(tweets_df, publishers_df, on='twitter_account', suffixes=('_tweets', '_publishers'))# for each publisher find the distribution of the engagement rates and engagement rates of the ones that are high in the analytuc feature\n",
    "    for liwc_feature in target_liwc_columns[:1]:\n",
    "        publishers_df[\"all_percentiles_shift_average_\"+liwc_feature] = np.nan\n",
    "        for publisher in tqdm(publishers_df[\"twitter_account\"].unique()):\n",
    "            original_engagement_rates = temp[temp[\"twitter_account\"] == publisher][\"engagement_rate_tweets\"]\n",
    "            high_analytic_engagement_rates = temp[(temp[\"twitter_account\"] == publisher) & (temp[liwc_feature] > target_liwc_medians[liwc_feature])][\"engagement_rate_tweets\"]\n",
    "            # if high_analytic_engagement_rates is empty then skip this publisher\n",
    "            if high_analytic_engagement_rates.shape[0] == 0:\n",
    "                continue\n",
    "            candidate_percentiles =[10,20,30,40,50,60,70,80,90]\n",
    "            candidate_percentiles =[50]\n",
    "            sum = 0\n",
    "            for  candidate_per in candidate_percentiles:\n",
    "                # find the percentile of the high_analytic_engagement_rates\n",
    "                percentile = np.percentile(high_analytic_engagement_rates, candidate_per)\n",
    "                # find the percentile_of_score of percentile in the original_engagement_rates\n",
    "                percentile_of_score = stats.percentileofscore(original_engagement_rates, percentile)\n",
    "                sum += (percentile_of_score-candidate_per)\n",
    "            sum = sum / len(candidate_percentiles)\n",
    "            publishers_df.loc[publishers_df[\"twitter_account\"] == publisher,\"all_percentiles_shift_average_\"+liwc_feature] = sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for liwc_feature in target_liwc_columns:\n",
    "    # how many are nan\n",
    "    print(f\"{liwc_feature} nan count:\",tweets_df[liwc_feature].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming publishers_df and tweets_df are already defined\n",
    "\n",
    "# Adding initial columns with default values\n",
    "for target_liwc in target_liwc_columns:\n",
    "    publishers_df[f\"high_{target_liwc}_engagement_rate_median_percentile\"] = -1000\n",
    "\n",
    "# Merging the tweets data with the publishers on 'twitter_account', differentiating same column names\n",
    "merged_df = pd.merge(tweets_df, publishers_df, on='twitter_account', suffixes=('_tweets', '_publishers'))\n",
    "\n",
    "for target_liwc in target_liwc_columns:\n",
    "    # Group by twitter_account and apply the percentile calculation\n",
    "    def percentile_calc(group):\n",
    "        sentiment_median = group[f\"high_{target_liwc}_engagement_rate\"].iloc[0]\n",
    "        values = np.sort(group['engagement_rate_tweets'].values)\n",
    "        return (np.searchsorted(values, sentiment_median) * 100 / len(group))-50\n",
    "\n",
    "    def effect_calc(group):\n",
    "        values = group[liwc_feature].values\n",
    "        # drop the nan values\n",
    "        \n",
    "        values = values[~np.isnan(values)]\n",
    "        this_median = np.median(values)\n",
    "        \n",
    "        high_values = group[group[liwc_feature] >=this_median]['engagement_rate_tweets'].values\n",
    "        low_values = group[group[liwc_feature] <this_median]['engagement_rate_tweets'].values\n",
    "        #print(high_values,low_values)\n",
    "        # drop the nan values\n",
    "        high_values = high_values[~np.isnan(high_values)]\n",
    "        low_values = low_values[~np.isnan(low_values)]\n",
    "        #print(high_values)\n",
    "        #print(low_values)\n",
    "        a = mannwhitneyu(high_values, low_values)\n",
    "        return a\n",
    "\n",
    "    # Apply the function to each group and map the results back to publishers_df\n",
    "    percentile_series = merged_df.groupby('twitter_account').apply(percentile_calc)\n",
    "    publishers_df = publishers_df.set_index('twitter_account')\n",
    "    publishers_df[f\"high_{target_liwc}_engagement_rate_median_percentile\"] = percentile_series\n",
    "    publishers_df.reset_index(inplace=True)\n",
    "    effect_series = merged_df.groupby('twitter_account').apply(effect_calc)\n",
    "    publishers_df = publishers_df.set_index('twitter_account')\n",
    "    publishers_df[f\"high_{target_liwc}_engagement_rate_effect\"] = effect_series\n",
    "    publishers_df.reset_index(inplace=True)\n",
    "    \n",
    "    # compute for each interaction type\n",
    "    COMPUTE_FOR_SUB_INTERACTIONS = False\n",
    "    if COMPUTE_FOR_SUB_INTERACTIONS:\n",
    "        for next_column in [\"like\", \"retweet\", \"reply\",\"quote\"]:\n",
    "            # Group by twitter_account and apply the percentile calculation\n",
    "            def percentile_calc(group):\n",
    "                sentiment_median = group[f\"high_{target_liwc}_{next_column}_engagement_rate\"].iloc[0]\n",
    "                values = np.sort(group[f'{next_column}_engagement_rate_tweets'].values)\n",
    "                return (np.searchsorted(values, sentiment_median) * 100 / len(group))-50\n",
    "\n",
    "            # Apply the function to each group and map the results back to publishers_df\n",
    "            percentile_series = merged_df.groupby('twitter_account').apply(percentile_calc)\n",
    "            publishers_df = publishers_df.set_index('twitter_account')\n",
    "            publishers_df[f\"high_{target_liwc}_{next_column}_engagement_rate_median_percentile\"] = percentile_series\n",
    "            publishers_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moivation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Reliable to \"Non-misinformation\" and Unreliable to \"Misinformation\" for the \"Reliability\" column\n",
    "publishers_df.loc[publishers_df['Reliability'] == 'Reliable', 'Reliability'] = \"Non-misinformation\"\n",
    "publishers_df.loc[publishers_df['Reliability'] == 'Unreliable', 'Reliability'] = \"Misinformation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the misinformaiton publishers wit the highest number of followers\n",
    "misinformation_publishers = publishers_df[publishers_df['Reliability'] == 'Misinformation']\n",
    "misinformation_publishers.sort_values(by=['followers_count'], ascending=False)[['twitter_account', 'followers_count','Bias']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shift in median for the breitbartnews account and the risk liwc feature\n",
    "tem = publishers_df[publishers_df['twitter_account'] == \"nytimes\"]\n",
    "print(tem[\"high_analytic_engagement_rate_median_percentile\"])\n",
    "# median of high analytic tweets\n",
    "temp2 = tweets_df[(tweets_df['twitter_account'] == \"nytimes\")]\n",
    "print(temp2['engagement_rate'].median())\n",
    "temp2 = temp2[(temp2['analytic'] > target_liwc_medians['analytic'])]\n",
    "print(temp2['engagement_rate'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cdf of the engagement rate for the accounts nytimes and wshingtonpost\n",
    "accounts = [\"nytimes\",\"foxnews\"]\n",
    "values = []\n",
    "names = []\n",
    "target_liwc = \"analytic\"\n",
    "for account in accounts:\n",
    "    values.append(tweets_df[tweets_df['twitter_account'] == account]['engagement_rate'].values)\n",
    "    names.append(f\"@{account}\")\n",
    "    values.append(tweets_df[(tweets_df['twitter_account'] == account) & (tweets_df[target_liwc]> target_liwc_medians[target_liwc] )]['engagement_rate'].values)\n",
    "    names.append(f\"@{account}\" + f\"_high_{target_liwc}\")\n",
    "# combine the two lists\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "plt.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "#plt.rcParams.update({'font.size': 24})\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bias_cclors = list(bias_color_dict.values())\n",
    "colors = [bias_cclors[0],bias_cclors[0],bias_cclors[-1],bias_cclors[-1]]\n",
    "i = 0\n",
    "for rates, name in zip(values, names):\n",
    "    # Calculate the empirical CDF\n",
    "    sorted_rates = np.sort(rates)\n",
    "    cdf = np.arange(1, len(sorted_rates) + 1) / len(sorted_rates)\n",
    "\n",
    "    if \"_high_\" not in name:\n",
    "        plt.plot(sorted_rates, cdf, label=name +\" (all tweets)\", color=colors[i], linewidth=3)\n",
    "        print(cdf[-1])\n",
    "        print(sorted_rates[0],sorted_rates[-1],sorted_rates[-10:])\n",
    "        i += 1\n",
    "    else:\n",
    "       # plot the dashed version\n",
    "        plt.plot(sorted_rates, cdf, label=name.split(\"_\")[0]+f\" ({target_liwc}al tweets)\", color=colors[i], linewidth=3, linestyle='--')\n",
    "        i+=1\n",
    "\n",
    "# add a horizontal line at 0.5\n",
    "#plt.axhline(y=0.5, color='black',linestyle='--' ,alpha=0.8, linewidth=2)\n",
    "#limit the x to the min and max of the data\n",
    "\n",
    "# put an arrow at the 0.5 point\n",
    "plt.xlabel(\"Engagement Rate\")\n",
    "\n",
    "#plt.xscale(\"log\")\n",
    "plt.xlim(0.0003,0.0035)\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend(fontsize=21,loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "# limit y to 0 to 1\n",
    "plt.ylim(0,1.00)\n",
    "\n",
    "# plto an arrow for where the last cdf has y of 0.5 to where the third cdf has y of 0.5\n",
    "ax = plt.gca()\n",
    "ticks = ax.get_xticks()\n",
    "# drop one out of two among the ticks\n",
    "ticks = ticks[::2]\n",
    "ticks = ticks[ticks != 0]  # Remove zero from the list of ticks\n",
    "ax.set_xticks(ticks)\n",
    "\n",
    "plt.annotate('', xy=(0.0012, 0.55), xytext=(0.0012, 0.5), arrowprops=dict(facecolor='black', shrink=0.01))\n",
    "\n",
    "plt.savefig(f\"./figs/cdfs/cdf_motivating_{target_liwc}.pdf\", dpi=300,  bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of tweets for nytimes and foxnews\n",
    "print(tweets_df[tweets_df['twitter_account'] == \"nytimes\"].shape)\n",
    "print(tweets_df[tweets_df['twitter_account'] == \"foxnews\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of followers for nytimes and foxnews\n",
    "print(publishers_df[publishers_df['twitter_account'] == \"nytimes\"]['followers_count'].values)\n",
    "print(publishers_df[publishers_df['twitter_account'] == \"foxnews\"]['followers_count'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median enagement rate of analytic posts of nytimes\n",
    "print(tweets_df[(tweets_df['twitter_account'] == \"nytimes\") & (tweets_df[target_liwc]> target_liwc_medians[target_liwc] )]['engagement_rate'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df[(tweets_df['twitter_account'] == \"nytimes\") ]['engagement_rate'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the publishers which are misinformation and have a high negative analytic median percentile\n",
    "tem = publishers_df[(publishers_df['Reliability'] == \"Misinformation\") & (publishers_df[\"high_analytic_engagement_rate_median_percentile\"] < 1)]\n",
    "# limit to the top 5 ones with the highest number of followers\n",
    "tem.sort_values(by=['followers_count'], ascending=False)[['twitter_account', 'followers_count','Bias']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#largest number of tweets and the twitter account\n",
    "twitter_account_counts = tweets_df['twitter_account'].value_counts()\n",
    "print(\"Largest number of tweets: \", max(twitter_account_counts))\n",
    "print(\"Twitter account with the largest number of tweets: \", twitter_account_counts[twitter_account_counts == max(twitter_account_counts)].index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = publishers_df.groupby(['Bias', 'Reliability']).agg({\n",
    "    'followers_count': 'sum',\n",
    "    'number_of_tweets': 'sum',\n",
    "    'like_count': 'sum',\n",
    "    'retweet_count': 'sum',\n",
    "    'quote_count': 'sum',\n",
    "    'reply_count': 'sum',\n",
    "    'total_enagement': 'sum',\n",
    "    'impression_count': 'sum',\n",
    "    'followers_count_mean': 'count'# used just to count the number of publishers in each group\n",
    "}).reset_index()\n",
    "# add a column for the number of publishers in each group\n",
    "grouped = grouped.rename(columns={'followers_count_mean': 'outlets'})\n",
    "\n",
    "grouped['engagement_rate'] = grouped['total_enagement'] / grouped['impression_count']\n",
    "\n",
    "# Sort dataframe based on your defined orders\n",
    "grouped['Bias'] = pd.Categorical(grouped['Bias'], sorted_bias_groups)\n",
    "grouped['Reliability'] = pd.Categorical(grouped['Reliability'], sorted_reliability_groups)\n",
    "grouped = grouped.sort_values(by=['Reliability', 'Bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity Calculation\n",
    "import pyperclip\n",
    "def calculate_intensity(value, max_value, min_value):\n",
    "    if max_value == min_value:\n",
    "        return 0\n",
    "    return int(100 * (value - min_value) / (max_value - min_value))\n",
    "\n",
    "def calculate_intensity(value, max_value, min_value):\n",
    "    if value > 1:\n",
    "        # Avoid logarithm of zero or negative values\n",
    "        value, max_value, min_value = max(1, value), max(1, max_value), max(1, min_value)\n",
    "        log_value, log_max, log_min = np.log(value), np.log(max_value), np.log(min_value)\n",
    "        if log_max == log_min:\n",
    "            return 0\n",
    "        return int(100 * (log_value - log_min) / (log_max - log_min))\n",
    "    else:\n",
    "        # non-logarithmic values\n",
    "        if max_value == min_value:\n",
    "            return 0\n",
    "        return int(100 * (value - min_value) / (max_value - min_value))\n",
    "    \n",
    "\n",
    "\n",
    "# Assuming 'grouped' is your DataFrame after aggregation and sorting\n",
    "\n",
    "# LaTeX helper function to format numbers\n",
    "\n",
    "\n",
    "# Create the LaTeX Table with multi-level headers\n",
    "latex_table = \"\"\"\n",
    "\\\\begin{table*}[htbp]\n",
    "  \\\\centering\n",
    "    \\\\caption{Statistics of the dataset categorized by political leaning and reliability type.}\n",
    "  \\\\begin{tabular}{l|l|rrrrrrrrrrr}\n",
    "    \\\\toprule\n",
    "    & Class & outlets & Followers & Tweets & Likes& Retweets& Replies& Quotes& Interactions & Impressions& Eng. rate \\\\\\\\\n",
    "    \\\\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Add Bias classes\n",
    "latex_table += \"    \\\\multirow{5}{*}{\\\\rotatebox[origin=c]{90}{Leaning}}\"\n",
    "for bias in sorted_bias_groups:\n",
    "    bias_data = grouped[grouped['Bias'] == bias]\n",
    "    latex_table += f\" & {bias}\"\n",
    "    for col in ['outlets', 'followers_count', 'number_of_tweets', 'like_count','retweet_count','reply_count','quote_count', 'total_enagement','impression_count','engagement_rate']:\n",
    "        if col == 'outlets':\n",
    "            latex_table += f\" & {bias_data[col].sum()}\"\n",
    "        elif col == \"engagement_rate\":\n",
    "            a = bias_data[\"total_enagement\"].sum()/bias_data[\"impression_count\"].sum()\n",
    "            latex_table += f\" & {y_fmt(a, decimals=3)}\"\n",
    "        else:\n",
    "            latex_table += f\" & {y_fmt(bias_data[col].sum())}\"\n",
    "    latex_table += \" \\\\\\\\\\n\"\n",
    "\n",
    "# Add Reliability classes\n",
    "latex_table += \"    \\\\midrule\\n\"\n",
    "latex_table += \"    \\\\multirow{2}{*}{\\\\rotatebox[origin=c]{90}{Rel.}}\"\n",
    "for reliability in sorted_reliability_groups:\n",
    "    reliability_data = grouped[grouped['Reliability'] == reliability]\n",
    "    reliability_renamed = reliability.replace(\"Misinformation\",\"Misinfo.\").replace(\"Non-misinformation\",\"Non-misinfo.\")\n",
    "    latex_table += f\" & {reliability_renamed}\"\n",
    "    for col in ['outlets', 'followers_count', 'number_of_tweets', 'like_count','retweet_count','reply_count','quote_count', 'total_enagement','impression_count','engagement_rate']:\n",
    "        if col == 'outlets':\n",
    "            latex_table += f\" & {reliability_data[col].sum()}\"\n",
    "        elif col == \"engagement_rate\":\n",
    "            a = reliability_data[\"total_enagement\"].sum()/reliability_data[\"impression_count\"].sum()\n",
    "            latex_table += f\" & {y_fmt(a, decimals=3)}\"\n",
    "        else:\n",
    "            latex_table += f\" & {y_fmt(reliability_data[col].sum())}\"\n",
    "    latex_table += \" \\\\\\\\\\n\"\n",
    "\n",
    "# Add Total row\n",
    "total_row = grouped[['outlets', 'followers_count', 'number_of_tweets', 'like_count','retweet_count','reply_count','quote_count', 'total_enagement','impression_count','engagement_rate']].sum()\n",
    "latex_table += \"    \\\\hline\\n\"\n",
    "latex_table += \"    \\\\multicolumn{2}{l|}{Total}\"\n",
    "for col in total_row.index:\n",
    "    if col == 'outlets':\n",
    "        latex_table += f\" & {int(total_row[col])}\"\n",
    "    elif col == \"engagement_rate\":\n",
    "        latex_table += f\"& {y_fmt(total_row['total_enagement']/total_row['impression_count'], decimals=3)}\"\n",
    "    else:\n",
    "        latex_table += f\" & {y_fmt(total_row[col])}\"\n",
    "latex_table += \" \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += \"\\\\bottomrule\\n\"\n",
    "latex_table += \"\\\\end{tabular}\\n\"\n",
    "latex_table += \"\\\\label{tab:dataset_statistics}\\n\"\n",
    "latex_table += \"\\\\end{table*}\\n\"\n",
    "\n",
    "# copy the latex_table to the clipboard\n",
    "\n",
    "pyperclip.copy(latex_table)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and median of the engagement rate\n",
    "print(\"Mean of the engagement rate: \", np.mean(tweets_df['engagement_rate']))\n",
    "print(\"Median of the engagement rate: \", np.median(tweets_df['engagement_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inport empirical_cdf\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_plots(group_values,groups_names,group_colors=None,y_log_scale=True,save_file_name=None,ylabel=None,show_whiskers=True,add_horizontal_line_at=None,show_means=True,rotate_x_ticks=None,varname=None,legened_loc='lower center'):\n",
    "    # replace Non-misinformation with Non-misinfo. in the group names and Misinformation with Misinfo.\n",
    "    groups_names = [x.replace(\"Non-misinformation\",\"Non-misinfo.\").replace(\"Misinformation\",\"Misinfo.\") for x in groups_names]\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "    plt.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "    plt.rcParams['pdf.fonttype'] = 42\n",
    "    plt.rcParams.update({'font.size': 24})\n",
    "    box_plot_positions = []\n",
    "    group_spacing = .8\n",
    "    width_of_box = 0.5\n",
    "    latest_position = 0\n",
    "    for i in range(len(group_values)):\n",
    "        box_plot_positions.append(latest_position)\n",
    "        latest_position += group_spacing\n",
    "        if i == 4:\n",
    "            latest_position += group_spacing-.2\n",
    "        if i == 6:\n",
    "            latest_position += group_spacing-.2\n",
    "    print(box_plot_positions)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    if show_whiskers == False:\n",
    "        # just show the interquartile range\n",
    "        box_plots = ax.boxplot(group_values, widths=width_of_box, positions=box_plot_positions, patch_artist=True, showfliers=True, notch=False, meanline=False, showmeans=show_means, whis=[25, 75])\n",
    "    else:\n",
    "        box_plots = ax.boxplot(group_values, widths=width_of_box, positions=box_plot_positions, patch_artist=True, showfliers=False, whis=[20, 80], notch=False, meanline=False, showmeans=show_means)\n",
    "        \n",
    "\n",
    "    \n",
    "    # apply the colors to the box plots\n",
    "    if group_colors is not None:\n",
    "        for patch, color in zip(box_plots['boxes'], group_colors):\n",
    "            patch.set_facecolor(color)\n",
    "    highlight_color = \"aquamarine\" # chartreuse\n",
    "\n",
    "    for median in box_plots['medians']:\n",
    "        median.set_color(highlight_color)  # Set the color of median line\n",
    "        #median.set_marker('o')\n",
    "        median.set_markerfacecolor(highlight_color)\n",
    "        # set the line width of the median\n",
    "        median.set_linewidth(3)\n",
    "        median.set_markersize(8)\n",
    "    if show_means:\n",
    "        for mean in box_plots['means']:\n",
    "            mean.set_color(highlight_color)  # Set the color of mean marker\n",
    "            mean.set_marker('o')\n",
    "            mean.set_markerfacecolor(highlight_color)\n",
    "            mean.set_markersize(8)\n",
    "            mean_patch = mlines.Line2D([], [], color=highlight_color, marker='o', linestyle='None', markersize=8, label='Mean')\n",
    "    ax.set_xticklabels(groups_names)\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(y_fmt))\n",
    "    \n",
    "    median_patch = mlines.Line2D([], [], color=highlight_color, linestyle='-', markersize=10, label='Median')\n",
    "\n",
    "    # change the y label font size\n",
    "    \n",
    "    # shift the y label down\n",
    "\n",
    "    # apply the scirentific notation to the y axis while the labels are floats\n",
    "    def y_fmt_box(x, pos):\n",
    "        \"\"\"\n",
    "        Custom formatting function for y-axis labels in scientific notation.\n",
    "        \"\"\"\n",
    "        if x == 0:\n",
    "            return \"0\"\n",
    "        exp = int(round(plt.log10(x)))\n",
    "        coeff = x / 10**exp\n",
    "        return fr\"${coeff:.2f} \\times 10^{exp}$\"\n",
    "    ax.yaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True, useOffset=False))\n",
    "    ax.yaxis.offsetText.set_fontsize(16)  # Adjust the font size of the exponent\n",
    "    ax.yaxis.major.formatter._useMathText = True\n",
    "    ax.yaxis.get_offset_text().set_position((0.1, 0.15))  # Adjust the position of the exponent\n",
    "\n",
    "    \n",
    "    if show_means: \n",
    "        #ax.legend(handles=[mean_patch, median_patch], labels=['Mean', 'Median'], loc='lower center', bbox_to_anchor=(0.5, 1.02), ncol=2, borderaxespad=0.0, fontsize='20')\n",
    "        ax.legend(handles=[mean_patch, median_patch], labels=['Mean', 'Median'],fontsize='20',loc=legened_loc,ncol=2)\n",
    "    else:\n",
    "        #ax.legend(handles=[median_patch], labels=['Median'], loc='lower center', bbox_to_anchor=(0.5, 1.02), ncol=2, borderaxespad=0.0, fontsize='28')\n",
    "        ax.legend(handles=[median_patch], labels=['Median'],fontsize='20',loc=legened_loc)\n",
    "    print(\"legend loc: \", legened_loc)\n",
    "    if add_horizontal_line_at is not None:\n",
    "        ax.axhline(y=add_horizontal_line_at, color='black', linestyle='--',linewidth=2)\n",
    "    if ylabel == None:\n",
    "        ax.set_ylabel(\"Engagement Rate\")\n",
    "    else:\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.yaxis.label.set_size(24)\n",
    "    # rotate the x axis labels\n",
    "    if rotate_x_ticks is not None:\n",
    "        plt.xticks(rotation=rotate_x_ticks, ha='right')\n",
    "    # decrease the x label font size\n",
    "    plt.xticks(fontsize=24)\n",
    "    # put the ha to the right\n",
    "    ax.tick_params(axis='x', which='major', pad=15) # shift the ticks with ha to the right\n",
    "    if y_log_scale:\n",
    "        plt.yscale(\"log\")\n",
    "    # increast the subticks marks size\n",
    "    ax.tick_params(axis='y', which='minor', length=5)\n",
    "    # shift the ticks with ha to the right\n",
    "    # xlabel font size\n",
    "    plt.savefig(f\"./figs/box_plots/box_plot_{save_file_name}.pdf\", dpi=300,  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def wilcoxon_signed_rank_test(data, constant=0):\n",
    "    \"\"\"\n",
    "    Perform the Wilcoxon Signed-Rank Test to compare a sample distribution to a constant value.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The distribution of numbers (sample data).\n",
    "    constant (float): The constant value to compare the distribution against.\n",
    "\n",
    "    Returns:\n",
    "    statistic (float): The test statistic.\n",
    "    p_value (float): The p-value for the test.\n",
    "    \"\"\"\n",
    "    # Calculate the differences between the data points and the constant\n",
    "    differences = np.array(data) - constant\n",
    "\n",
    "    # Perform the Wilcoxon Signed-Rank Test\n",
    "    statistic, p_value1 = wilcoxon(differences, alternative='less')\n",
    "    \n",
    "    statistic, p_value2 = wilcoxon(differences, alternative='greater')\n",
    "\n",
    "\n",
    "    return p_value1,p_value2,np.median(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the {sentiment}_engagement_rate_median_percentile\n",
    "for column in target_liwc_columns:\n",
    "    print(column)\n",
    "    if column== \"analytic\":\n",
    "        legened_loc = 'lower left'\n",
    "    elif column == \"perception\":\n",
    "        legened_loc = 'upper right'\n",
    "    else:\n",
    "        legened_loc = 'lower right'\n",
    "    column1 = f\"high_{column}_engagement_rate_median_percentile\"\n",
    "    if COMBINED_DIMENSION == None:\n",
    "        save_file_name=f\"{column}_engagement_rate_median_percentile\"\n",
    "    else:\n",
    "        save_file_name=f\"{column}_{COMBINED_DIMENSION}_engagement_rate_median_percentile\"\n",
    "    \n",
    "    median_percentile_values = []\n",
    "    for bias in sorted_bias_groups:\n",
    "        temp = publishers_df[publishers_df['Bias'] == bias]\n",
    "        # drop the infinities\n",
    "        #temp = temp[temp[f\"{sentiment}_engagement_rate_median_percentile\"] != -1000]\n",
    "        # add the non nan values\n",
    "        a = temp[column1].values\n",
    "        a = a[~np.isnan(a)]\n",
    "        median_percentile_values.append(a)\n",
    "        #print(sentiment, bias,wilcoxon_signed_rank_test(median_percentile_values[-1], constant=0))\n",
    "    for reliability in sorted_reliability_groups:\n",
    "        temp = publishers_df[publishers_df['Reliability'] == reliability]\n",
    "        # drop the infinities\n",
    "        #temp = temp[temp[f\"{sentiment}_engagement_rate_median_percentile\"] != -1000]\n",
    "        # drop the nan values\n",
    "        a = temp[column1].values\n",
    "        a = a[~np.isnan(a)]\n",
    "        median_percentile_values.append(a)        #print(sentiment, reliability,wilcoxon_signed_rank_test(median_percentile_values[-1], constant=0))\n",
    "    # add also all the data\n",
    "    # add the non nan values\n",
    "    a = publishers_df[column1].values\n",
    "    a = a[~np.isnan(a)]\n",
    "    median_percentile_values.append(a)\n",
    "    # drop the nan values\n",
    "\n",
    "    \n",
    "\n",
    "    #plot_empirical_cdfs(median_percentile_values, sorted_bias_groups, f\"{sentiment}_engagement_rate_median_percentile\",min_y_cdf=y_cdf_min,max_x=100,x_log_scale=False,y_log_scale=False,colors = list(bias_color_dict.values()),xlabel=f\"{sentiment} tweets engagement rate median percentile\")\n",
    "    # also plot the box plot\n",
    "\n",
    "    plot_box_plots(median_percentile_values,sorted_bias_groups+sorted_reliability_groups+[\"All\"],y_log_scale=False,group_colors = list(bias_color_dict.values())+list(reliability_color_dict.values())+[\"white\"],save_file_name=save_file_name,ylabel=f\"Median Percentiles Shift\",add_horizontal_line_at=0,show_means=False,show_whiskers=True,rotate_x_ticks=45,varname=column,legened_loc=legened_loc)\n",
    "    #plot_box_plots(median_percentile_values, sorted_bias_groups,y_log_scale=False,group_colors = list(bias_color_dict.values()),save_file_name=f\"{column}_engagement_rate_median_percentile\",ylabel=f\"Increase in median percentile\",add_horizontal_line_at=0,show_means=False,show_whiskers=True,rotate_x_ticks=45)\n",
    "    \n",
    "    \n",
    "    # also plot the cdf of the median percentile\n",
    "    #plot_empirical_cdfs(median_percentile_values, sorted_bias_groups, f\"{sentiment}_engagement_rate_median_percentile\",min_y_cdf=0,max_x=100,x_log_scale=False,y_log_scale=False,colors = list(bias_color_dict.values()),xlabel=f\"{sentiment} tweets engagement rate median percentile\")\n",
    "    # also run the kruscall wallis test\n",
    "    #print(\"Kruscall wallis test for \", sentiment)\n",
    "    \n",
    "    # run the wilcoxon signed rank test for each group for the nonnan values\n",
    "    # print the medians of all groups \n",
    "    for i,group in enumerate(sorted_bias_groups+sorted_reliability_groups+[\"All\"]):\n",
    "        print(group,np.median(median_percentile_values[i]))\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    #for i,group in enumerate(sorted_bias_groups+sorted_reliability_groups+[\"All\"]):\n",
    "    #    values = median_percentile_values[i]\n",
    "    #    values = values[~np.isnan(values)]\n",
    "    #    print(group, wilcoxon_signed_rank_test(values, constant=0))\n",
    "    #print(\"------------------------\")\n",
    "\n",
    "    #run_kruskal_wallis(median_percentile_values, sorted_bias_groups+sorted_reliability_groups+[\"All\"])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the {sentiment}_engagement_rate_median_percentile\n",
    "for column in target_liwc_columns[:1]:\n",
    "    print(column)\n",
    "    if column== \"analytic\":\n",
    "        legened_loc = 'lower left'\n",
    "    elif column == \"perception\":\n",
    "        legened_loc = 'upper right'\n",
    "    else:\n",
    "        legened_loc = 'lower right'\n",
    "    column1 = \"all_percentiles_shift_average_\"+column\n",
    "\n",
    "    median_percentile_values = []\n",
    "    for bias in sorted_bias_groups:\n",
    "        temp = publishers_df[publishers_df['Bias'] == bias]\n",
    "        # drop the infinities\n",
    "        #temp = temp[temp[f\"{sentiment}_engagement_rate_median_percentile\"] != -1000]\n",
    "        # add the non nan values\n",
    "        a = temp[column1].values\n",
    "        a = a[~np.isnan(a)]\n",
    "        median_percentile_values.append(a)\n",
    "        #print(sentiment, bias,wilcoxon_signed_rank_test(median_percentile_values[-1], constant=0))\n",
    "    for reliability in sorted_reliability_groups:\n",
    "        temp = publishers_df[publishers_df['Reliability'] == reliability]\n",
    "        # drop the infinities\n",
    "        #temp = temp[temp[f\"{sentiment}_engagement_rate_median_percentile\"] != -1000]\n",
    "        # drop the nan values\n",
    "        a = temp[column1].values\n",
    "        a = a[~np.isnan(a)]\n",
    "        median_percentile_values.append(a)        #print(sentiment, reliability,wilcoxon_signed_rank_test(median_percentile_values[-1], constant=0))\n",
    "    # add also all the data\n",
    "    # add the non nan values\n",
    "    a = publishers_df[column1].values\n",
    "    a = a[~np.isnan(a)]\n",
    "    median_percentile_values.append(a)\n",
    "    # drop the nan values\n",
    "\n",
    "    \n",
    "\n",
    "    #plot_empirical_cdfs(median_percentile_values, sorted_bias_groups, f\"{sentiment}_engagement_rate_median_percentile\",min_y_cdf=y_cdf_min,max_x=100,x_log_scale=False,y_log_scale=False,colors = list(bias_color_dict.values()),xlabel=f\"{sentiment} tweets engagement rate median percentile\")\n",
    "    # also plot the box plot\n",
    "\n",
    "    plot_box_plots(median_percentile_values,sorted_bias_groups+sorted_reliability_groups+[\"All\"],y_log_scale=False,group_colors = list(bias_color_dict.values())+list(reliability_color_dict.values())+[\"white\"],save_file_name=f\"{column}_engagement_rate_percentile_deviations_mean\",ylabel=f\"Average Percentiles Shift\",add_horizontal_line_at=0,show_means=False,show_whiskers=True,rotate_x_ticks=45,varname=column,legened_loc=legened_loc)\n",
    "    #plot_box_plots(median_percentile_values, sorted_bias_groups,y_log_scale=False,group_colors = list(bias_color_dict.values()),save_file_name=f\"{column}_engagement_rate_median_percentile\",ylabel=f\"Increase in median percentile\",add_horizontal_line_at=0,show_means=False,show_whiskers=True,rotate_x_ticks=45)\n",
    "    \n",
    "    \n",
    "    # also plot the cdf of the median percentile\n",
    "    #plot_empirical_cdfs(median_percentile_values, sorted_bias_groups, f\"{sentiment}_engagement_rate_median_percentile\",min_y_cdf=0,max_x=100,x_log_scale=False,y_log_scale=False,colors = list(bias_color_dict.values()),xlabel=f\"{sentiment} tweets engagement rate median percentile\")\n",
    "    # also run the kruscall wallis test\n",
    "    #print(\"Kruscall wallis test for \", sentiment)\n",
    "    \n",
    "    # run the wilcoxon signed rank test for each group for the nonnan values\n",
    "    # print the medians of all groups \n",
    "    for i,group in enumerate(sorted_bias_groups+sorted_reliability_groups+[\"All\"]):\n",
    "        print(group,np.median(median_percentile_values[i]))\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    #for i,group in enumerate(sorted_bias_groups+sorted_reliability_groups+[\"All\"]):\n",
    "    #    values = median_percentile_values[i]\n",
    "    #    values = values[~np.isnan(values)]\n",
    "    #    print(group, wilcoxon_signed_rank_test(values, constant=0))\n",
    "    #print(\"------------------------\")\n",
    "\n",
    "    #run_kruskal_wallis(median_percentile_values, sorted_bias_groups+sorted_reliability_groups+[\"All\"])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Percentiles shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the {sentiment}_engagement_rate_median_percentile\n",
    "for column in target_liwc_columns[:1]:\n",
    "    print(column)\n",
    "    column1 = \"all_percentiles_shift_average_\"+column\n",
    "\n",
    "    median_percentile_values = []\n",
    "    for bias in sorted_bias_groups:\n",
    "        temp = publishers_df[publishers_df['Bias'] == bias]\n",
    "        # drop the infinities\n",
    "        #temp = temp[temp[f\"{sentiment}_engagement_rate_median_percentile\"] != -1000]\n",
    "        # add the non nan values\n",
    "        a = temp[column1].values\n",
    "        a = a[~np.isnan(a)]\n",
    "        median_percentile_values.append(a)\n",
    "        #print(sentiment, bias,wilcoxon_signed_rank_test(median_percentile_values[-1], constant=0))\n",
    "    for reliability in sorted_reliability_groups:\n",
    "        temp = publishers_df[publishers_df['Reliability'] == reliability]\n",
    "        # drop the infinities\n",
    "        #temp = temp[temp[f\"{sentiment}_engagement_rate_median_percentile\"] != -1000]\n",
    "        # drop the nan values\n",
    "        a = temp[column1].values\n",
    "        a = a[~np.isnan(a)]\n",
    "        median_percentile_values.append(a)        #print(sentiment, reliability,wilcoxon_signed_rank_test(median_percentile_values[-1], constant=0))\n",
    "    # add also all the data\n",
    "    # add the non nan values\n",
    "    a = publishers_df[column1].values\n",
    "    a = a[~np.isnan(a)]\n",
    "    median_percentile_values.append(a)\n",
    "    # drop the nan values\n",
    "\n",
    "    \n",
    "\n",
    "    #plot_empirical_cdfs(median_percentile_values, sorted_bias_groups, f\"{sentiment}_engagement_rate_median_percentile\",min_y_cdf=y_cdf_min,max_x=100,x_log_scale=False,y_log_scale=False,colors = list(bias_color_dict.values()),xlabel=f\"{sentiment} tweets engagement rate median percentile\")\n",
    "    # also plot the box plot\n",
    "    \n",
    "    plot_box_plots(median_percentile_values,sorted_bias_groups+sorted_reliability_groups+[\"All\"],y_log_scale=False,group_colors = list(bias_color_dict.values())+list(reliability_color_dict.values())+[\"white\"],save_file_name=f\"{column}_engagement_rate_percentile_deviations_mean\",ylabel=f\"Average Percentiles Shift\",add_horizontal_line_at=0,show_means=False,show_whiskers=True,rotate_x_ticks=45,varname=column)\n",
    "    #plot_box_plots(median_percentile_values, sorted_bias_groups,y_log_scale=False,group_colors = list(bias_color_dict.values()),save_file_name=f\"{column}_engagement_rate_median_percentile\",ylabel=f\"Increase in median percentile\",add_horizontal_line_at=0,show_means=False,show_whiskers=True,rotate_x_ticks=45)\n",
    "    \n",
    "    \n",
    "    # also plot the cdf of the median percentile\n",
    "    #plot_empirical_cdfs(median_percentile_values, sorted_bias_groups, f\"{sentiment}_engagement_rate_median_percentile\",min_y_cdf=0,max_x=100,x_log_scale=False,y_log_scale=False,colors = list(bias_color_dict.values()),xlabel=f\"{sentiment} tweets engagement rate median percentile\")\n",
    "    # also run the kruscall wallis test\n",
    "    #print(\"Kruscall wallis test for \", sentiment)\n",
    "    \n",
    "    # run the wilcoxon signed rank test for each group for the nonnan values\n",
    "    # print the medians of all groups \n",
    "    for i,group in enumerate(sorted_bias_groups+sorted_reliability_groups+[\"All\"]):\n",
    "        print(group,np.median(median_percentile_values[i]))\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "    #for i,group in enumerate(sorted_bias_groups+sorted_reliability_groups+[\"All\"]):\n",
    "    #    values = median_percentile_values[i]\n",
    "    #    values = values[~np.isnan(values)]\n",
    "    #    print(group, wilcoxon_signed_rank_test(values, constant=0))\n",
    "    #print(\"------------------------\")\n",
    "\n",
    "    #run_kruskal_wallis(median_percentile_values, sorted_bias_groups+sorted_reliability_groups+[\"All\"])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(median_percentile_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 in temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_liwc_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevelance of the different sentiments among different bias groups\n",
    "for liwc_feature in target_liwc_columns:\n",
    "    \n",
    "    print(liwc_feature)\n",
    "    percentage_values = []\n",
    "    for bias in sorted_bias_groups:\n",
    "        temp = publishers_df[publishers_df['Bias'] == bias]\n",
    "        # drop the infinities\n",
    "        temp = temp[temp[f\"high_{liwc_feature}_percentage\"] != -1000]\n",
    "        # drop the nan values\n",
    "\n",
    "        \n",
    "        percentage_values.append(temp[f\"high_{liwc_feature}_percentage\"].values)\n",
    "    \n",
    "    for reliability in sorted_reliability_groups:\n",
    "        temp = publishers_df[publishers_df['Reliability'] == reliability]\n",
    "        # drop the infinities\n",
    "        temp = temp[temp[f\"high_{liwc_feature}_percentage\"] != -1000]\n",
    "        # drop the nan values\n",
    "\n",
    "        \n",
    "        percentage_values.append(temp[f\"high_{liwc_feature}_percentage\"].values)\n",
    "        \n",
    "    if liwc_feature in [\"analytic\",\"perception\"]:\n",
    "        legened_loc = 'lower left'\n",
    "    elif liwc_feature == \"clout\":\n",
    "        legened_loc = 'upper center'\n",
    "    else:\n",
    "        legened_loc = 'lower center'\n",
    "\n",
    "        \n",
    "    cdf_labels = sorted_bias_groups+sorted_reliability_groups\n",
    "    #plot_empirical_cdfs(percentage_values, cdf_labels, f\"Percentage\",max_x=100,x_log_scale=True,y_log_scale=False,colors = list(bias_color_dict.values()),xlabel=f\"high{liwc_feature} percentage\")\n",
    "    plot_box_plots(percentage_values, cdf_labels,y_log_scale=False,group_colors = list(bias_color_dict.values())+list(reliability_color_dict.values()),save_file_name=f\"high_{liwc_feature}_percentage\",ylabel=f\"Percentage\",show_means=True,show_whiskers=True,rotate_x_ticks=45,varname=liwc_feature,legened_loc=legened_loc)\n",
    "    \n",
    "    for i,group in enumerate(sorted_bias_groups+sorted_reliability_groups):\n",
    "        print(group,np.median(percentage_values[i]))\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "\n",
    "    run_kruskal_wallis(percentage_values, sorted_bias_groups+sorted_reliability_groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
